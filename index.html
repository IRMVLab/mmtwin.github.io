<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Novel Diffusion Models for Multimodal 3D Hand Trajectory Prediction.">
  <meta name="keywords" content="Hand Trajectory Prediction, Egocentric Vision, Hand-Object Interaction">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Novel Diffusion Models for Multimodal 3D Hand Trajectory Prediction</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>



<section class="section">
  <div class="hero-body">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">MMTwin: Novel Diffusion Models for Multimodal 3D Hand Trajectory Prediction</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://bit-mjy.github.io/">Junyi Ma</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://cogito2012.github.io/homepage/">Wentao Bao</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://github.com/BIT-XJY">Jingyi Xu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://github.com">Guanzhong Sun</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=DvrngV4AAAAJ&hl=zh-CN">Xieyuanli Chen</a><sup>4</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=en&user=q6AY9XsAAAAJ">Hesheng Wang</a><sup>1</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Shanghai Jiao Tong University,</span>
            <span class="author-block"><sup>2</sup>Meta Reality Labs,</span>
            <span class="author-block"><sup>3</sup>China University of Mining and Technology,</span>
            <span class="author-block"><sup>4</sup>National University of Defense Technology</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2409.02638"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/IRMVLab/MMTwin"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/IRMVLab/MMTwin"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>
          </div>
          
          <div class="column has-text-centered">
            <img src="./static/images/motivation.png"
                 class="architecture-image"
                 alt="architecture-image image."
                 style="width: 40%; display: block; margin: 0 auto;"/>
            <h2 class="content has-text-justified" style="width: 55%; margin: 0 auto;">
              What about considering <strong>the synergy between hand movements and headset camera egomotion</strong> within the future interaction process?
            </h2>
          </div>

          <div class="column has-text-centered">
              <video id="teaser" autoplay muted loop playsinline height="50%" 
                     style="width: 50%; box-shadow: 0 4px 8px 0 rgba(0,0,0,0.2), 0 6px 20px 0 rgba(0,0,0,0.19);">
                <source src="./static/videos/compressed_mmtwin_video.mp4" type="video/mp4">
              </video>
              <h2 class="content has-text-justified" style="width: 55%; margin: 0 auto;">
                MMTwin incorporates <strong>RGB images, point clouds, text prompts, and past hand waypoints</strong> to coherently predict future camera egomotion and 3D hand trajectories in egocentric views.
              </h2>
          </div>

          <div class="column has-text-centered">
            <div class="columns is-centered has-text-centered">
              <div class="column is-four-fifths">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                  <p>
                  <strong>We present novel diffusion models (MMTwin) for multimodal 3D hand trajectory prediction.</strong> MMTwin is designed to absorb multimodal information as input encompassing 2D RGB images, 3D point clouds, past hand waypoints, and text prompt. Besides, two latent diffusion models, the egomotion diffusion and the HTP diffusion as twins, are integrated into MMTwin to predict camera egomotion and future hand trajectories concurrently. We propose a novel hybrid Mamba-Transformer module as the denoising model of the HTP diffusion to better fuse multimodal features. The experimental results on three publicly available datasets and our self-recorded data demonstrate that our proposed MMTwin can predict plausible future 3D hand trajectories compared to the state-of-the-art baselines, and generalizes well to unseen environments.</p>
                </div>
              </div>
            </div>
          </div>

          <div class="column has-text-centered">
            <div class="columns is-centered has-text-centered">
              <div class="column is-four-fifths">
                <h2 class="title is-3">MMTwin Architecture</h2>
                  <img src="./static/images/mmtwin_arch.png"
                   class="architecture-image"
                   alt="architecture-image image."
                   style="width: 60%; display: block; margin: 0 auto;"/>
                <div class="content has-text-justified">
                  <p>
                  Our proposed MMTwin (a) extracts features from multimodal data, and (b) decouples predictions of future camera egomotion features and 3D hand trajectories by novel twin diffusion models. The vanilla Mamba (VM) is used for denoising in the egomotion diffusion. We further design a new denoising model in HTP diffusion with (c) a hybrid Mamba-Transformer module (HMTM), encompassing the egomotion-aware Mamba (EAM) blocks and (d) the structure-aware Transformer (SAT).
                </div>
              </div>
            </div>
          </div>

          <div class="columns is-centered">

            <div class="column">
              <div class="content">
                <video id="exp1" autoplay muted loop playsinline height="60%">
                  <source src="./static/videos/sample1_nomdss.mp4" type="video/mp4">
                </video>
              </div>
            </div>
      
            <div class="column">
              <div class="content">
                <video id="exp2" autoplay muted loop playsinline height="60%">
                  <source src="./static/videos/sample2_nomdss.mp4" type="video/mp4">
                </video>
              </div>
            </div>
      
            <div class="column">
              <div class="content">
                <video id="exp3" autoplay muted loop playsinline height="60%">
                  <source src="./static/videos/sample3_nomdss.mp4" type="video/mp4">
                </video>
              </div>
            </div>
      
        </div>

          

          <div class="column has-text-centered">
              <h2 class="title is-3">Multifinger Predictions</h2>
              <video id="teaser" autoplay muted loop playsinline height="50%" 
                     style="width: 50%; box-shadow: 0 4px 8px 0 rgba(0,0,0,0.2), 0 6px 20px 0 rgba(0,0,0,0.19);">
                <source src="./static/videos/multi_finger_pred.mp4" type="video/mp4">
              </video>
              <h2 class="content has-text-justified" style="width: 55%; margin: 0 auto;">
                MMTwin can well predict 3D movements of multiple fingers, which exhibits the potential for transfer to robotic skills (we are currently working on).
              </h2>
          </div>
          
          
        </div>
    </div>
  </div>
</section>












  
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{ma2024madiff,
      title={MADiff: Motion-Aware Mamba Diffusion Models for Hand Trajectory Prediction on Egocentric Videos}, 
      author={Junyi Ma and Xieyuanli Chen and Wentao Bao and Jingyi Xu and Hesheng Wang},
      year={2024},
      eprint={2409.02638},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2409.02638}, 
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/abs/2409.02638">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/BIT-MJY" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>



</body>
</html>
