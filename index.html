<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Novel Diffusion Models for Multimodal 3D Hand Trajectory Prediction.">
  <meta name="keywords" content="Hand Trajectory Prediction, Egocentric Vision, Hand-Object Interaction">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Novel Diffusion Models for Multimodal 3D Hand Trajectory Prediction</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Novel Diffusion Models for Multimodal 3D Hand Trajectory Prediction</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://bit-mjy.github.io/">Junyi Ma</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://cogito2012.github.io/homepage/">Wentao Bao</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://github.com/BIT-XJY">Jingyi Xu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://github.com">Guanzhong Sun</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=DvrngV4AAAAJ&hl=zh-CN">Xieyuanli Chen</a><sup>4</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=en&user=q6AY9XsAAAAJ">Hesheng Wang</a><sup>1</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Shanghai Jiao Tong University,</span>
            <span class="author-block"><sup>2</sup>Meta Reality Labs,</span>
            <span class="author-block"><sup>3</sup>China University of Mining and Technology,</span>
            <span class="author-block"><sup>4</sup>National University of Defense Technology</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2409.02638"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/IRMVLab/MMTwin"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/IRMVLab/MMTwin"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="50%">
        <source src="./static/videos/compressed_mmtwin_video.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        MMTwin incorporates RGB images, point clouds, text prompts, and past hand waypoints to accurately predict future 3D hand trajectories in egocentric views.
      </h2>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          **We present novel diffusion models (MMTwin) for multimodal 3D hand trajectory prediction.** MMTwin is designed to absorb multimodal information as input encompassing 2D RGB images, 3D point clouds, past hand waypoints, and text prompt. Besides, two latent diffusion models, the egomotion diffusion and the HTP diffusion as twins, are integrated into MMTwin to predict camera egomotion and future hand trajectories concurrently. We propose a novel hybrid Mamba-Transformer module as the denoising model of the HTP diffusion to better fuse multimodal features. The experimental results on three publicly available datasets and our self-recorded data demonstrate that our proposed MMTwin can predict plausible future 3D hand trajectories compared to the state-of-the-art baselines, and generalizes well to unseen environments. The code and pretrained models will be released at https://github.com/IRMVLab/MMTwin.          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section" style="text-align: center;" >
  <h2 class="title is-3" style="text-align: center;">
    MADiff Architecture
  </h2>
  <img src="./static/images/madiff_architecture.png"
       class="architecture-image"
       alt="architecture-image image."
       style="width: 60%; display: block; margin: 0 auto;"/>
</section>



<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

    <!-- Semantic Extraction -->
      <div class="column">
        <h2 class="title is-3">Semantic Extraction</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              We use the foundation model for extracting semantic features sensitive to hand-scenario relationships. Visual grounding allows us to use text prompt to achieve task-specific. 
            </p>
            <video id="mdss" autoplay muted loop playsinline height="60%">
              <source src="./static/videos/grounding.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    <!--/ MDSS. -->
    
      <!-- Denoising to Future Traj -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Denoising to Future Trajs</h2>
          <p>
            Motion-aware Mamba is proposed to gradually denoise latent features in the latent diffusion to predict accurate hand waypoints. 
          </p>
          <video id="train" autoplay muted loop playsinline height="60%">
            <source src="./static/videos/denoising2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ Training Strategy. -->
    </div>
</div>
</section>
  

  
<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

    <!-- MDSS. -->
      <div class="column">
        <h2 class="title is-3">Motion-Driven Selective Scan</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              MDSS is proposed in our motion-aware Mamba to achieve diffusion denoising under egomotion guidance. 
            </p>
            <video id="mdss" autoplay muted loop playsinline height="60%">
              <source src="./static/videos/mdss.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    <!--/ MDSS. -->
    
      <!-- Training Strategy. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Training Strategy</h2>
          <p>
            We use angle and length constraints to optimize the directionality and stabability of predicted hand trajectories.
          </p>
          <video id="train" autoplay muted loop playsinline height="60%">
            <source src="./static/videos/supervision.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ Training Strategy. -->
    </div>
</div>
</section>


<section class="section">
  <h2 class="title is-3" style="text-align: center;">
    MADiff vs. AR and iter-NAR paradigms
  </h2>
  <img src="./static/images/paradigm.png"
       class="architecture-image"
       alt="architecture-image image."
       style="width: 40%; display: block; margin: 0 auto;"/>
  <h2 class="subtitle has-text-centered">
        MADiff inference with Motion-Aware Mamba and CDC operation
  </h2>
</section>

  
  <section class="section">
  <div class="container is-max-desktop">

    <h2 class="title is-3" style="text-align: center;">
      Additional HTP Results
    </h2>

    <div class="columns is-centered">

      <div class="column">
        <div class="content">
          <video id="exp1" autoplay muted loop playsinline height="60%">
            <source src="./static/videos/exp1.mp4" type="video/mp4">
          </video>
        </div>
      </div>

      <div class="column">
        <div class="content">
          <video id="exp2" autoplay muted loop playsinline height="60%">
            <source src="./static/videos/exp2.mp4" type="video/mp4">
          </video>
        </div>
      </div>

      <div class="column">
        <div class="content">
          <video id="exp3" autoplay muted loop playsinline height="60%">
            <source src="./static/videos/exp3.mp4" type="video/mp4">
          </video>
        </div>
      </div>

    </div>

    <div class="columns is-centered">

      <div class="column">
        <div class="content">
          <video id="exp4" autoplay muted loop playsinline height="60%">
            <source src="./static/videos/exp4.mp4" type="video/mp4">
          </video>
        </div>
      </div>

      <div class="column">
        <div class="content">
          <video id="exp5" autoplay muted loop playsinline height="60%">
            <source src="./static/videos/exp5.mp4" type="video/mp4">
          </video>
        </div>
      </div>

      <div class="column">
        <div class="content">
          <video id="exp6" autoplay muted loop playsinline height="60%">
            <source src="./static/videos/exp6.mp4" type="video/mp4">
          </video>
        </div>
      </div>

    </div>
    <p style="text-align: center;">
      The starting point of the predicted trajectory does not align with the hand in the image because here we use the first frame of the video clip as the prediction canvas, while the predicted trajectory’s starting point is at the timestamp close to the last frame of the video. Please refer to more results of comparison with baselines in our <a href="https://arxiv.org/abs/2409.02638">our paper</a>.
    </p>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">

    <h2 class="title is-3" style="text-align: center;">
      Multiple Samples by MADiff
    </h2>

    <div class="columns is-centered">

      <div class="column">
        <div class="content">
          <video id="exp1" autoplay muted loop playsinline height="60%">
            <source src="./static/videos/sample1.mp4" type="video/mp4">
          </video>
        </div>
      </div>

      <div class="column">
        <div class="content">
          <video id="exp2" autoplay muted loop playsinline height="60%">
            <source src="./static/videos/sample2.mp4" type="video/mp4">
          </video>
        </div>
      </div>

      <div class="column">
        <div class="content">
          <video id="exp3" autoplay muted loop playsinline height="60%">
            <source src="./static/videos/sample3.mp4" type="video/mp4">
          </video>
        </div>
      </div>

    </div>

    <div class="columns is-centered">
      
      <div class="column">
        <div class="content">
          <video id="exp3" autoplay muted loop playsinline height="60%">
            <source src="./static/videos/sample4.mp4" type="video/mp4">
          </video>
        </div>
      </div>

      <div class="column">
        <div class="content">
          <video id="exp3" autoplay muted loop playsinline height="60%">
            <source src="./static/videos/sample5.mp4" type="video/mp4">
          </video>
        </div>
      </div>

      <div class="column">
        <div class="content">
          <video id="exp3" autoplay muted loop playsinline height="60%">
            <source src="./static/videos/sample6.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      
    </div>
    <p style="text-align: center;">
      MADiff follows a generative scheme and thus can generate trajectory clusters by multiple sampling from Gaussian noise.
    </p>
  </div>
</section>


  
 <section class="section">
  <div class="container is-max-desktop">

    <h2 class="title is-3" style="text-align: center;">
      Multiple Samples by Models without MDSS
    </h2>

    <div class="columns is-centered">

      <div class="column">
        <div class="content">
          <video id="exp1" autoplay muted loop playsinline height="60%">
            <source src="./static/videos/sample1_nomdss.mp4" type="video/mp4">
          </video>
        </div>
      </div>

      <div class="column">
        <div class="content">
          <video id="exp2" autoplay muted loop playsinline height="60%">
            <source src="./static/videos/sample2_nomdss.mp4" type="video/mp4">
          </video>
        </div>
      </div>

      <div class="column">
        <div class="content">
          <video id="exp3" autoplay muted loop playsinline height="60%">
            <source src="./static/videos/sample3_nomdss.mp4" type="video/mp4">
          </video>
        </div>
      </div>

    </div>

    <div class="columns is-centered">
      
      <div class="column">
        <div class="content">
          <video id="exp3" autoplay muted loop playsinline height="60%">
            <source src="./static/videos/sample4_nomdss.mp4" type="video/mp4">
          </video>
        </div>
      </div>

      <div class="column">
        <div class="content">
          <video id="exp3" autoplay muted loop playsinline height="60%">
            <source src="./static/videos/sample5_nomdss.mp4" type="video/mp4">
          </video>
        </div>
      </div>

      <div class="column">
        <div class="content">
          <video id="exp3" autoplay muted loop playsinline height="60%">
            <source src="./static/videos/sample6_nomdss.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      
    </div>
    <p style="text-align: center;">
      The models agnostic to camera egomotion tend to accumulate prediction errors due to motion gaps increasing along the time axis. The final position and overall shape similarity are also not as good as those of the model with MDSS.
    </p>
  </div>
</section> 



  
<!-- specific verb -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3" style="text-align: center;">
      Text Prompt Tuning
    </h2>

    <div class="columns is-centered">
      <div class="column">
        <div class="columns is-multiline">
          <div class="column is-half">
            <div class="content">
              <video id="exp1" autoplay muted loop playsinline width="100%">
                <source src="./static/videos/without_verb_crop.mp4" type="video/mp4">
              </video>
              <p style="text-align: center; font-size: 0.9em; font-family: Verdana, sans-serif;">
                “hand”
              </p>
            </div>
          </div>
          <div class="column is-half">
            <div class="content">
              <video id="exp2" autoplay muted loop playsinline width="100%">
                <source src="./static/videos/with_verb_crop.mp4" type="video/mp4">
              </video>
              <p style="text-align: center; font-size: 0.9em; font-family: Verdana, sans-serif;">
                “hand, which is scooping”
              </p>
            </div>
          </div>
          <div class="column is-half">
            <div class="content">
              <video id="exp3" autoplay muted loop playsinline width="100%">
                <source src="./static/videos/without_verb_crop_2.mp4" type="video/mp4">
              </video>
              <p style="text-align: center; font-size: 0.9em; font-family: Verdana, sans-serif;">
                “hand”
              </p>
            </div>
          </div>
          <div class="column is-half">
            <div class="content">
              <video id="exp4" autoplay muted loop playsinline width="100%">
                <source src="./static/videos/with_verb_crop_2.mp4" type="video/mp4">
              </video>
              <p style="text-align: center; font-size: 0.9em; font-family: Verdana, sans-serif;">
                “hand, which is throwing”
              </p>
            </div>
          </div>
        </div>
      </div>

      <div class="column is-one-third">
        <div class="content">
          <figure style="margin-bottom: 0;">
            <img src="./static/images/verb3.png" alt="Image description" style="width: 100%;">
          </figure>
          <p style="text-align: center; font-size: 0.9em; font-family: Verdana, sans-serif;">
               Text prompt tuning helps to extract action-specific semantics, thus improving corresponding HTP performance.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


  
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{ma2024madiff,
      title={MADiff: Motion-Aware Mamba Diffusion Models for Hand Trajectory Prediction on Egocentric Videos}, 
      author={Junyi Ma and Xieyuanli Chen and Wentao Bao and Jingyi Xu and Hesheng Wang},
      year={2024},
      eprint={2409.02638},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2409.02638}, 
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/abs/2409.02638">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/BIT-MJY" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>



</body>
</html>
